{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0c734b-d3f2-43ea-bc2b-68d0505347ec",
   "metadata": {},
   "source": [
    "# Autoencoder: toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7919879-7506-48d2-b129-b8e6214463cc",
   "metadata": {},
   "source": [
    "Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f50117-b50f-4c38-a728-4ce69a8057e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /srv/conda/envs/notebook/lib/python3.7/site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.15 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (9.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: typing-extensions in /srv/conda/envs/notebook/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decb2f7-8734-49fd-8a43-a01454ff133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688087d-4620-44ca-9afe-2f4751697b83",
   "metadata": {},
   "source": [
    "### Autoencoder model 'bone structure'\n",
    "\n",
    "Below is the minimal structure of components of an Autoencoder. Full code is contained in `autoendocer_utils.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe3c6f-40be-4237-aa5c-c30df534f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Ecoder(torch.nn.Module): # torch.nn.Module is the base class for NN\n",
    "#     def __init__():\n",
    "#         pass\n",
    "    \n",
    "#     def forward():\n",
    "#         pass\n",
    "    \n",
    "# class Decoder(torch.nn.Module):\n",
    "#     def __init__():\n",
    "#         pass\n",
    "    \n",
    "#     def forward():\n",
    "#         pass\n",
    "    \n",
    "# class Autoencoder(torch.nn.Module):\n",
    "#     def __init__():\n",
    "#         self.encoder = Encoder()\n",
    "#         self.decoder = Decoder()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "    \n",
    "#     def get_latent_space(self, x):\n",
    "#         return self.encoder(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce59514-cc89-4e73-9727-f685db271a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder_utils import Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ebb7c-c11e-4793-ac72-dfcd26703c64",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a03b2b-21d9-4c62-89b6-fa55031cf5f2",
   "metadata": {},
   "source": [
    "We will consider very simple training data: solid background and a higher-value diagonal. A random noise is added on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed25c6-3c5a-487f-b2b7-949372fa1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder_utils import generate_dataset, create_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67bc86-e219-4b35-8783-8b8f7895b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 100 # size of training set\n",
    "N_test = 10 # size of test set\n",
    "p = 5 # input dimension\n",
    "\n",
    "X_train = generate_dataset(N_train, p)\n",
    "X_test = generate_dataset(N_test, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdb2f2-5f4d-44e8-a9b5-60a0e9a2f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 200, figsize = (2,2))\n",
    "plt.title('Example of the training data')\n",
    "plt.imshow(X_test[2, 0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb36d7-136b-4a85-a892-75fd0d77c8b0",
   "metadata": {},
   "source": [
    "Set up the model and the training attributes: cost function (the MSE), its optimizer (Adam), learning rate, batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9160d-fc30-47fe-ba28-b6c2573b823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(p, 3,4, 5)\n",
    "cost_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam( model.parameters(),lr = 0.0001)\n",
    "batch = 4\n",
    "train_cost  = []\n",
    "test_cost = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3a774-3be5-468e-849f-3508bf5c639f",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b074d9-3a8c-479a-9bda-f2eb26e28982",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    \n",
    "    temp_train_cost =[]\n",
    "    for j in range(0,N_train-batch, batch):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_train_batch = X_train[j:j+batch, :, :, :]\n",
    "        x_hat = model(x_train_batch)\n",
    "        cost = cost_function(x_train_batch, x_hat)\n",
    "        \n",
    "        cost.backward()\n",
    "        optimizer.step()    \n",
    "        \n",
    "        temp_train_cost.append(cost.detach().numpy())\n",
    "        \n",
    "    train_cost.append(np.mean(temp_train_cost))\n",
    "    test_x_hat = model(X_test)\n",
    "    test_cost.append(cost_function(test_x_hat, X_test).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c4f1b-586b-4790-80b0-e4c0cc4a6c56",
   "metadata": {},
   "source": [
    "Visualize the evolution of the cost functions for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8107cea-4bc9-4c16-974d-0c3444c17710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 150)\n",
    "plt.title('Cost Function')\n",
    "plt.plot(train_cost, label = 'training')\n",
    "plt.plot(test_cost, label = 'test')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb3273-a152-41e4-bd5c-6abb35a92343",
   "metadata": {},
   "source": [
    "### How normal inputs are reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1d96c-5705-4081-b42a-4bbed4b93a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, dpi = 200, figsize = (5, 10))\n",
    "\n",
    "y = create_example(p)\n",
    "y_hat = model(y.reshape(1,1,p,p)).detach().numpy().reshape((p,p))\n",
    "\n",
    "ax[0].imshow(y, vmin = 0, vmax = 1)\n",
    "ax[0].set_title('input')\n",
    "\n",
    "ax[1].imshow(y_hat, vmin = 0, vmax = 1)\n",
    "ax[1].set_title('output')\n",
    "\n",
    "print(f'MSE of decoded image is {np.mean((y.detach().numpy()-y_hat)**2):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618652f-efca-409c-80af-fcca6d7bff03",
   "metadata": {},
   "source": [
    "### How abnormal inputs are reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d22518-dd74-4f17-a42e-138424bf6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder_utils import generate_abnormal_1, generate_abnormal_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42d0ae-76a8-45da-a64c-53d3d3e8f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, dpi = 200, figsize = (5, 10))\n",
    "\n",
    "y = generate_abnormal_1(p, with_noise = True)\n",
    "y_hat = model(y.reshape(1,1,p,p)).detach().numpy().reshape((p,p))\n",
    "\n",
    "ax[0].imshow(y, vmin = 0, vmax = 1)\n",
    "ax[0].set_title('input')\n",
    "\n",
    "ax[1].imshow(y_hat, vmin = 0, vmax = 1)\n",
    "ax[1].set_title('output')\n",
    "\n",
    "print(f'MSE of decoded image is {np.mean((y.detach().numpy()-y_hat)**2):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f4dc1-b5bd-48cc-9708-12cbd7464edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, dpi = 200, figsize = (5, 10))\n",
    "\n",
    "y = generate_abnormal_2(p, with_noise = True)\n",
    "y_hat = model(y.reshape(1,1,p,p)).detach().numpy().reshape((p,p))\n",
    "\n",
    "ax[0].imshow(y, vmin = 0, vmax = 1)\n",
    "ax[0].set_title('input')\n",
    "\n",
    "ax[1].imshow(y_hat, vmin = 0, vmax = 1)\n",
    "ax[1].set_title('output')\n",
    "\n",
    "print(f'MSE of decoded image is {np.mean((y.detach().numpy()-y_hat)**2):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510e692-c761-467c-87ce-40f337451c05",
   "metadata": {},
   "source": [
    "Now, let us look at the distributions of the latent coordinates and the recontruction errors. We generate 1000 of normal examples and record their latent coordinates and the MSE between the input and the output. Then we do the same for the abnormal examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf349d86-5a76-45f4-8d98-0c912b3e4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS = []\n",
    "MSE_normal = []\n",
    "MSE_abnormal = []\n",
    "for k in range(1000):\n",
    "    y = create_example(p)\n",
    "    LS.append(model.get_latent_space(y.reshape(1,1,p,p)).detach().numpy().flatten())\n",
    "    MSE_normal.append(torch.mean((y-model(y.reshape(1,1,p,p)))**2).detach().numpy().flatten()[0])\n",
    "    \n",
    "# and one anomaly\n",
    "y = generate_abnormal_1(p, with_noise = True)\n",
    "LS.append(model.get_latent_space(y.reshape(1,1,p,p)).detach().numpy().flatten())\n",
    "MSE_abnormal.append(torch.mean((y-model(y.reshape(1,1,p,p)))**2).detach().numpy().flatten()[0])\n",
    "\n",
    "\n",
    "y = generate_abnormal_2(p, with_noise = True)\n",
    "LS.append(model.get_latent_space(y.reshape(1,1,p,p)).detach().numpy().flatten())\n",
    "MSE_abnormal.append(torch.mean((y-model(y.reshape(1,1,p,p)))**2).detach().numpy().flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5edf6c-9283-4f64-916c-5687e8fe94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 200, figsize = (6,3))\n",
    "plt.title('Distribution of the MSE')\n",
    "_= plt.hist(MSE_normal, bins = 10, density=True, label = 'normal')\n",
    "_= plt.hist(MSE_abnormal, bins = 2, density=True, label = 'abnormal')\n",
    "_ = plt.yticks([])\n",
    "plt.ylabel('Density of states')\n",
    "plt.xlabel('MSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866de03-882f-4b5b-9874-75d236cae871",
   "metadata": {},
   "source": [
    "Let us look at the distribution of the latent coordinates using pairwise plot.\n",
    "It is not guaranteed that the abnormal examples would be outliers for every latent coordinate. But with a good autoencoder model, there should be at least one coordinate, along which the abnormal examples are far away from the normal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b65398-e623-4e09-bf68-1db37dd7537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constract a DataFrame for plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(LS)\n",
    "df['normal'] = 'normal'\n",
    "df.iloc[-1, -1] = 'abnormal_2'\n",
    "df.iloc[-2, -1] = 'abnormal_1'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "g = sns.pairplot(df, hue = 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571c079-075d-446b-b9e6-80adc8e7c1a7",
   "metadata": {},
   "source": [
    "# Autoencoder for XPCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327c79d-225c-4cee-8db2-aab8adaeaf0a",
   "metadata": {},
   "source": [
    "In this section we look at the use of latent space of an eutoencoder model for the two time correlation functions (2TCF) for XPCS experiments. The model is pre-trained and only the latent space coordinates are used here. The details of model training can be found at:\n",
    "* https://doi.org/10.11578/dc.20210704.1\n",
    "* https://doi.org/10.1038/s41598-021-93747-y\n",
    "* https://doi.org/10.48550/arXiv.2201.07889\n",
    "\n",
    "For anomaly detection, we consider two algorithms: DBSCAN clustering and Isolation Forest.\n",
    "We consider two experiments where instrumentation instabilities happend. For a researcher, it is easy to identify the anomalous 2TCFs. We will see how one can teach a computer to recognize such anomalies. \n",
    "\n",
    "For each experiment, first two time series are used to generate (through dow-sampling) many examples of stable 2TCFs. From all other time series, only two down-sampled examples are considered. Remember, the anomalies need to be rare to be recognized. \n",
    "\n",
    "Since it is easy to recognize the anomaly for a researcher, we have the true labels avaiable to us to evalue the algorithm performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1fad0-d576-49f6-9275-420674f41ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca91d34-a22d-4569-b521-1482241485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiled.client import from_uri\n",
    "c = from_uri(\"https://tiled-demo.blueskyproject.io/api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70babf-1e2e-456b-be5b-5b2409e4a5ed",
   "metadata": {},
   "source": [
    "## Experiment #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1400dcae-d9f0-4e61-b1c1-3e5d4d932c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiled.client import show_logs\n",
    "# show_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0c1ea-055a-4ef6-9dbc-fad108487234",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_stable = list(c['um2022']['tatiana']['csx_stable']['experiment1'])\n",
    "files_unstable = list(c['um2022']['tatiana']['csx_unstable']['experiment1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e318b54-16bb-40f5-8f85-7bc174875cf7",
   "metadata": {},
   "source": [
    "Plot the 2TCFs. Top row are stable series and bottom raw are unstable series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09bf0f-23d9-469e-a11c-352985e30805",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize = (15,8))\n",
    "for j in range(3):\n",
    "    x = c['um2022']['tatiana']['csx_stable']['experiment1'][files_stable[j]].read()\n",
    "    w = x.shape[1]\n",
    "    ax[0, j].imshow(x.reshape(w,w), vmin = 1.05, vmax = 1.20, origin = 'lower')\n",
    "    ax[0,j].set(xlabel = r'$t_1$, frames', ylabel = r'$t_2$, frames')\n",
    "    \n",
    "for j in range(2):\n",
    "    x = c['um2022']['tatiana']['csx_unstable']['experiment1'][files_unstable[j]].read()\n",
    "    w = x.shape[1]\n",
    "    ax[1, j].imshow(x.reshape((w,w)), vmin = 1.0, vmax = 1.1, origin = 'lower')\n",
    "    ax[1,j].set(xlabel = r'$t_1$, frames', ylabel = r'$t_2$, frames')\n",
    "    \n",
    "ax[-1,-1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbe14b-93ab-43e2-89e5-3a8de139fe39",
   "metadata": {},
   "source": [
    "Now let us look at the latent coordinate of all time series (calculated separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2f925-7321-4076-b3ae-f215c5f0f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stable = pd.DataFrame(c['um2022']['tatiana']['tables']['experiment1_first_stable'].read())\n",
    "df_unstable = pd.DataFrame(c['um2022']['tatiana']['tables']['experiment1_next_scans'].read())\n",
    "df = pd.concat([df_stable,df_unstable], ignore_index=True, sort=False)\n",
    "df = df.drop(columns = [df.columns[0]])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2538e-b3c1-4aad-a030-7c852b86f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "g = sns.pairplot(df[:], plot_kws={'alpha': 0.5, 'edgecolor':'none'}, hue = 'stability')\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.get_yaxis().set_label_coords(-0.5,0.5)\n",
    "    \n",
    "for j in range(8):\n",
    "    for i in range(8):\n",
    "        g.axes[i,j].set_xlim((-10,10))\n",
    "        g.axes[i,j].set_ylim((-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39645fa3-98bf-46b6-8251-e8409ba4b3f5",
   "metadata": {},
   "source": [
    "Now let us look closer at a particular plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d92913-c108-4929-8c9e-464af237b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'lc7', y='lc8', data=df, hue = 'stability', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fa251-fca0-4977-b1ac-01abb24fff6f",
   "metadata": {},
   "source": [
    "First, we see how DBSCAN assign the data. One can vary min_samples and reachability distance (eps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0587edb-367c-43b1-9654-3641cd1e0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:8].values\n",
    "\n",
    "cluster = DBSCAN(min_samples = 10, eps= 3)\n",
    "labels = cluster.fit_predict(X)\n",
    "df['dbscan_labels'] = labels\n",
    "sns.lmplot(x = 'lc7', y='lc8', data=df, hue = 'dbscan_labels', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc9d7a-585b-4545-8ae8-f622dc526d5d",
   "metadata": {},
   "source": [
    "Then look at the same data with Isolation forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155b4a5-357d-4495-9721-9eb13ee0832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = 4/X.shape[0]\n",
    "clf = IsolationForest(random_state=0, max_features = 2, contamination = contamination).fit(X)\n",
    "labels = clf.predict(X)\n",
    "df['iforest_labels'] = labels\n",
    "sns.lmplot(x = 'lc7', y='lc8', data=df, hue = 'iforest_labels', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63accdba-5344-40a5-86d5-ec78df0821fc",
   "metadata": {},
   "source": [
    "With Isolation Forest we can 'profile' the entire dataset by `anomaly score`. The higher the score, the more likely the sample is an anomaly. Depending on the assumed contamination level, we can find a proper threshold for the separating the normal data from outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2e757-c362-46a6-a50f-375fc32fc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(-clf.score_samples(X), bins = 25)\n",
    "plt.title('Anomaly Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a138e-2586-4316-a423-c0cc9221effe",
   "metadata": {},
   "source": [
    "## Experiment #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e52297-4f34-494b-af08-4478b7868182",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_stable = list(c['um2022']['tatiana']['csx_stable']['experiment2'])\n",
    "files_unstable = list(c['um2022']['tatiana']['csx_unstable']['experiment2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b4e89-5bf0-4a76-8b10-9bbcc6813180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize = (15,8))\n",
    "for j in range(3):\n",
    "    x = c['um2022']['tatiana']['csx_stable']['experiment2'][files_stable[j]].read()\n",
    "    w = x.shape[1]\n",
    "    ax[0, j].imshow(x.reshape(w,w), vmin = 1.3, vmax = 1.7, origin = 'lower')\n",
    "    ax[0,j].set(xlabel = r'$t_1$, frames', ylabel = r'$t_2$, frames')\n",
    "    \n",
    "for j in range(2):\n",
    "    x = c['um2022']['tatiana']['csx_unstable']['experiment2'][files_unstable[j]].read()\n",
    "    w = x.shape[1]\n",
    "    ax[1, j].imshow(x.reshape((w,w)), vmin = 1.4, vmax = 1.6, origin = 'lower')\n",
    "    ax[1,j].set(xlabel = r'$t_1$, frames', ylabel = r'$t_2$, frames')\n",
    "    \n",
    "ax[-1,-1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916173b5-de23-472b-90de-8d1c8c677f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stable = pd.DataFrame(c['um2022']['tatiana']['tables']['experiment2_first_stable'].read())\n",
    "df_unstable = pd.DataFrame(c['um2022']['tatiana']['tables']['experiment2_next_scans'].read())\n",
    "df = pd.concat([df_stable,df_unstable], ignore_index=True, sort=False)\n",
    "df = df.drop(columns = [df.columns[0]])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473f800-0b20-47ef-8c48-4fb55d5b65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "g = sns.pairplot(df[:], plot_kws={'alpha': 0.7, 'edgecolor':'none'}, hue = 'stability')\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    ax.get_yaxis().set_label_coords(-0.5,0.5)\n",
    "    \n",
    "for j in range(8):\n",
    "    for i in range(8):\n",
    "        g.axes[i,j].set_xlim((-25,25))\n",
    "        g.axes[i,j].set_ylim((-25,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aab150-ab73-4d78-bfb4-2f9930de024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'lc4', y='lc5', data=df, hue = 'stability', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707ce6d-284e-4ee2-89c9-ebceed02bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = DBSCAN(min_samples = 10, eps= 3)\n",
    "labels = cluster.fit_predict(df.iloc[:, 1:8].values)\n",
    "df['dbscan_labels'] = labels\n",
    "sns.lmplot(x = 'lc4', y='lc5', data=df, hue = 'dbscan_labels', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6640b-72ae-46e2-b440-08b0b9af61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:8].values\n",
    "\n",
    "contamination = 4/X.shape[0]\n",
    "clf = IsolationForest(random_state=0, max_features = 2, contamination = contamination).fit(X)\n",
    "labels = clf.predict(X)\n",
    "df['iforest_labels'] = labels\n",
    "sns.lmplot(x = 'lc4', y='lc5', data=df, hue = 'iforest_labels', fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd00c6d-9415-44cc-b69e-50ce543794cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(-clf.score_samples(X), bins = 25)\n",
    "plt.title('Anomaly Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4952f5-a981-49d2-ba4b-d116087ef397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
